"""
Phase 1.4: Column Mapping & Data Quality Validation
====================================================
Map 331 survey columns to 5 theme schemas and validate data quality

Process:
1. Load survey data with coordinates
2. Identify column structure (English/Arabic pairs)
3. Map to existing themes (Water, Energy, Food, General_Info, Regenerative_Agriculture)
4. Normalize column names (remove trailing colons, standardize spacing)
5. Handle checkbox arrays and multi-value fields
6. Validate data types and ranges
7. Export clean theme-specific CSV files
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime

# Column mapping to themes
# Based on survey structure analysis and existing theme schemas
THEME_MAPPING = {
    'Water': [
        '10.Ù…Ø§ Ù‡Ù…Ø§ Ø§Ù„Ù…Ø­ØµÙˆÙ„Ø§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ§Ù† Ø§Ù„Ù„Ø°Ø§Ù† ØªØ²Ø±Ø¹Ù‡Ù…Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ø³Ù†Ø© (Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø£Ùˆ Ø§Ù„Ø¯Ø®Ù„)ØŸ',
        '11.ÙÙŠ Ø£ÙŠ Ø£Ø´Ù‡Ø± ÙŠØªÙ… Ø²Ø±Ø§Ø¹Ø© Ù‡Ø°ÙŠÙ† Ø§Ù„Ù…Ø­ØµÙˆÙ„ÙŠÙ†ØŸ',
        '12.Ù…Ø§ Ù‡Ùˆ Ù†ÙˆØ¹ Ø§Ù„Ø±ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù‡Ø°ÙŠÙ† Ø§Ù„Ù…Ø­ØµÙˆÙ„ÙŠÙ†ØŸ',
        '13.Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…ÙŠØ§Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„Ø±ÙŠØŸ',
        '14.Ù…Ø§ Ù‡Ùˆ Ù…ØµØ¯Ø± Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ø°ÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡ Ù„Ù„Ø±ÙŠ ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©ØŸ',
        '15.ÙƒÙ… Ù…Ø±Ø© ØªÙ‚ÙˆÙ… Ø¨Ø±ÙŠ Ù‡Ø°ÙŠÙ† Ø§Ù„Ù…Ø­ØµÙˆÙ„ÙŠÙ† ÙÙŠ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ØŸ',
        '16.ÙƒÙŠÙ ØªÙ‚ÙŠÙ‘Ù… ØªÙˆÙØ± Ø§Ù„Ù…ÙŠØ§Ù‡ Ø®Ù„Ø§Ù„ Ù…ÙˆØ³Ù… Ø§Ù„Ø²Ø±Ø§Ø¹Ø©ØŸ',
        '17.Ù‡Ù„ ØªÙˆØ§Ø¬Ù‡ Ù†Ù‚ØµÙ‹Ø§ ÙÙŠ Ø§Ù„Ù…ÙŠØ§Ù‡ ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª Ù…Ù† Ø§Ù„Ø³Ù†Ø©ØŸ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù†Ø¹Ù…ØŒ Ø­Ø¯Ø¯ Ø§Ù„Ø£Ø´Ù‡Ø±.',
    ],
    'Energy': [
        '14.Ù…Ø§ Ù‡Ùˆ Ù…ØµØ¯Ø± Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ø°ÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡ Ù„Ù„Ø±ÙŠ ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©ØŸ',
        '18.Ù…Ø§ Ù‡ÙŠ ÙƒÙ…ÙŠØ© Ø§Ù„Ø·Ø§Ù‚Ø© (Ø¨Ø§Ù„ÙƒÙŠÙ„ÙˆÙˆØ§Øª Ø£Ùˆ Ø§Ù„Ù„ØªØ±Ø§Øª Ù…Ù† Ø§Ù„ÙˆÙ‚ÙˆØ¯) Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø®Ù„Ø§Ù„ Ù…ÙˆØ³Ù… Ø§Ù„Ø°Ø±ÙˆØ©ØŸ',
    ],
    'Food': [
        '10.Ù…Ø§ Ù‡Ù…Ø§ Ø§Ù„Ù…Ø­ØµÙˆÙ„Ø§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ§Ù† Ø§Ù„Ù„Ø°Ø§Ù† ØªØ²Ø±Ø¹Ù‡Ù…Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ø³Ù†Ø© (Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø£Ùˆ Ø§Ù„Ø¯Ø®Ù„)ØŸ',
        '19.Ù…Ø§ Ù‡Ùˆ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„ÙƒÙ„ Ù…Ù† Ø§Ù„Ù…Ø­ØµÙˆÙ„ÙŠÙ† (Ø¨Ø§Ù„Ø·Ù† Ø£Ùˆ Ø§Ù„ÙˆØ­Ø¯Ø§Øª)ØŸ',
        '54.Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØºØ°Ø§Ø¦ÙŠØ© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ†ØªØ¬Ù‡Ø§ Ø£Ùˆ ØªØ´ØªØ±ÙŠ Ù…Ù† Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠÙŠÙ†ØŸ',
        '55.Ù…Ø§ Ù‡ÙŠ Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø´Ø§Ø±ÙƒÙŠÙ† ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø¤ÙˆÙ†Ø© ÙÙŠ Ù…Ù†Ø·Ù‚ØªÙƒØŸ',
        '63.Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ø¯ÙˆØ§Ø¬Ù† Ø£Ùˆ Ù…Ø§Ø´ÙŠØ© Ø£Ø®Ø±Ù‰ØŸ',
        '64.Ù…Ø§ Ù‡ÙŠ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ±Ø¨ÙŠÙ‡Ø§ØŸ',
        '65.ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø·ÙŠÙˆØ± Ø§Ù„ØªÙŠ ØªØ±Ø¨ÙŠÙ‡Ø§ØŸ',
        '66.Ù…Ø§ Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù„Ù Ø§Ù„Ø°ÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡ Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø¯ÙˆØ§Ø¬Ù† Ø£Ùˆ Ø§Ù„Ù…Ø§Ø´ÙŠØ©ØŸ',
    ],
    'General_Info': [
        '4.Ø§Ù„Ù‚Ø±ÙŠØ©:',
        '5.Ù‡Ù„ ØªÙ…ØªÙ„Ùƒ Ø£Ø±Ø¶Ø§Ù‹ Ø²Ø±Ø§Ø¹ÙŠØ©ØŸ',
        '6.Ø£ÙŠÙ† ØªÙ‚Ø¹ Ø£Ø±Ø¶Ùƒ Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©ØŸ',
        '7.ÙƒÙ… Ø¹Ø¯Ø¯ Ù‚Ø·Ø¹ Ø§Ù„Ø£Ø±Ø¶ Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ…Ù„ÙƒÙ‡Ø§ØŸ',
        '8.Ù…Ø§ Ù‡Ùˆ Ø­Ø¬Ù… Ø§Ù„Ø­ÙŠØ§Ø²Ø© Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒØŸ',
        '9.Ù…Ø§ Ù‡Ùˆ Ù†ÙˆØ¹ Ø§Ù„ØªØ±Ø¨Ø© ÙÙŠ Ø£Ø±Ø¶ÙƒØŸ',
        '20.Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ ØªØºÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ø® Ø¹Ù„Ù‰ Ù…Ø¯Ù‰ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ù‚Ù„ÙŠÙ„Ø© Ø§Ù„Ù…Ø§Ø¶ÙŠØ© ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø±Ø§Ø¹Ø©ØŸ',
        '21.Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§Ø­Ø¸ØªÙ‡Ø§ØŸ',
        '22.ÙƒÙŠÙ Ø£Ø«Ø±Øª Ù‡Ø°Ù‡ Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø¹Ù„Ù‰ Ø²Ø±Ø§Ø¹ØªÙƒØŸ',
        'X',
        'Y',
    ],
    'Regenerative_Agriculture': [
        '29.Ù…Ø§ Ù…Ø¯Ù‰ Ù…Ø¹Ø±ÙØªÙƒ Ø¨Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø²Ø±Ø§Ø¹Ø© Ø§Ù„ØªØ¬Ø¯ÙŠØ¯ÙŠØ©ØŸ',
        '30.Ù‡Ù„ Ø³Ù…Ø¹Øª Ø¹Ù† Ø§Ù„Ø²Ø±Ø§Ø¹Ø© Ø§Ù„ØªØ¬Ø¯ÙŠØ¯ÙŠØ© Ù…Ù† Ù‚Ø¨Ù„ØŸ',
        '31.Ù‡Ù„ Ø­Ø¶Ø±Øª Ø£ÙŠ Ø¨Ø±Ø§Ù…Ø¬ ØªØ¯Ø±ÙŠØ¨ÙŠØ© Ø£Ùˆ ÙˆØ±Ø´ Ø¹Ù…Ù„ Ø­ÙˆÙ„ Ø§Ù„Ø²Ø±Ø§Ø¹Ø© Ø§Ù„ØªØ¬Ø¯ÙŠØ¯ÙŠØ©ØŸ',
        '32.Ù‡Ù„ ØªÙ…Ø§Ø±Ø³ Ø§Ù„Ø²Ø±Ø§Ø¹Ø© Ø§Ù„ØªØ¬Ø¯ÙŠØ¯ÙŠØ©ØŸ',
        '33.Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙŠ ØªØ·Ø¨Ù‚Ù‡Ø§ Ù…Ù† Ø§Ù„Ø²Ø±Ø§Ø¹Ø© Ø§Ù„ØªØ¬Ø¯ÙŠØ¯ÙŠØ©ØŸ',
        '38.Ù…Ø§ Ù‡ÙŠ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„ØªØ±Ø¨Ø©ØŸ',
        '39.Ù…Ø§ Ù…Ø¯Ù‰ Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ù…Ø¯Ø© Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠØ©ØŸ',
        '43.ÙƒÙŠÙ ØªÙ‚ÙˆÙ… Ø¨Ù…ÙƒØ§ÙØ­Ø© Ø§Ù„Ø¢ÙØ§ØªØŸ',
        '44.Ù…Ø§ Ù…Ø¯Ù‰ Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¨ÙŠØ¯Ø§Øª Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠØ©ØŸ',
    ]
}

# Common fields across all themes
COMMON_FIELDS = [
    '1.Ø§Ø³Ù… Ø§Ù„Ù…ÙØ³ØªØ¬ÙŠØ¨:',
    '4.Ø§Ù„Ù‚Ø±ÙŠØ©:',
    'X',
    'Y'
]


def load_survey_with_coords():
    """Load survey data with coordinates"""
    input_file = Path('data/MZSurvey farmers ENGLISH_with_coords.csv')
    
    if not input_file.exists():
        raise FileNotFoundError(f"Coordinate file not found: {input_file}")
    
    df = pd.read_csv(input_file, encoding='utf-8')
    print(f"âœ“ Loaded survey data: {len(df)} rows, {len(df.columns)} columns")
    
    return df


def normalize_column_name(col):
    """Normalize column name (remove trailing colons, extra spaces)"""
    if pd.isna(col):
        return col
    
    col = str(col).strip()
    
    # Remove trailing colon and space
    if col.endswith(':'):
        col = col[:-1].strip()
    
    # Normalize multiple spaces
    col = ' '.join(col.split())
    
    return col


def identify_column_pairs(df):
    """Identify English-Arabic column pairs"""
    columns = df.columns.tolist()
    pairs = []
    unpaired_en = []
    unpaired_ar = []
    
    i = 0
    while i < len(columns):
        col = columns[i]
        
        # Check if next column exists and might be a pair
        if i + 1 < len(columns):
            next_col = columns[i + 1]
            
            # Simple heuristic: if both start with same number (e.g., "4." and "4.")
            # or if one has Arabic and one doesn't
            col_has_arabic = any('\u0600' <= c <= '\u06FF' for c in str(col))
            next_has_arabic = any('\u0600' <= c <= '\u06FF' for c in str(next_col))
            
            if col_has_arabic != next_has_arabic:
                # Likely a pair
                if col_has_arabic:
                    pairs.append({'en': next_col, 'ar': col})
                else:
                    pairs.append({'en': col, 'ar': next_col})
                i += 2
                continue
        
        # Unpaired column
        if any('\u0600' <= c <= '\u06FF' for c in str(col)):
            unpaired_ar.append(col)
        else:
            unpaired_en.append(col)
        
        i += 1
    
    return pairs, unpaired_en, unpaired_ar


def map_columns_to_themes(df):
    """Map columns to themes based on content"""
    
    theme_columns = {
        'Water': [],
        'Energy': [],
        'Food': [],
        'General_Info': [],
        'Regenerative_Agriculture': []
    }
    
    unmapped_columns = []
    
    for col in df.columns:
        col_normalized = normalize_column_name(col)
        mapped = False
        
        # Check if column is in any theme mapping
        for theme, theme_cols in THEME_MAPPING.items():
            if col in theme_cols or col_normalized in theme_cols:
                theme_columns[theme].append(col)
                mapped = True
                break
        
        # Check common fields
        if not mapped and (col in COMMON_FIELDS or col_normalized in COMMON_FIELDS):
            for theme in theme_columns:
                if col not in theme_columns[theme]:
                    theme_columns[theme].append(col)
            mapped = True
        
        if not mapped:
            unmapped_columns.append(col)
    
    return theme_columns, unmapped_columns


def validate_data_quality(df):
    """Validate data quality and generate report"""
    
    issues = []
    warnings = []
    stats = {}
    
    # Check coordinates
    if 'X' in df.columns and 'Y' in df.columns:
        null_coords = df[df['X'].isna() | df['Y'].isna()]
        if len(null_coords) > 0:
            issues.append(f"Missing coordinates: {len(null_coords)} rows")
        
        # Check coordinate ranges (Lebanon bounds)
        if 'X' in df.columns:
            x_out_of_bounds = df[(df['X'] < 35.0) | (df['X'] > 37.0)]
            if len(x_out_of_bounds) > 0:
                warnings.append(f"Longitude out of bounds: {len(x_out_of_bounds)} rows")
        
        if 'Y' in df.columns:
            y_out_of_bounds = df[(df['Y'] < 33.0) | (df['Y'] > 35.0)]
            if len(y_out_of_bounds) > 0:
                warnings.append(f"Latitude out of bounds: {len(y_out_of_bounds)} rows")
        
        stats['coordinates'] = {
            'total_rows': len(df),
            'with_coords': int((df['X'].notna() & df['Y'].notna()).sum()),
            'missing_coords': int((df['X'].isna() | df['Y'].isna()).sum()),
            'lon_range': [float(df['X'].min()), float(df['X'].max())],
            'lat_range': [float(df['Y'].min()), float(df['Y'].max())]
        }
    
    # Check village names
    if '4.Ø§Ù„Ù‚Ø±ÙŠØ©:' in df.columns:
        village_col = '4.Ø§Ù„Ù‚Ø±ÙŠØ©:'
        villages = df[village_col].value_counts()
        stats['villages'] = {
            'unique_count': int(len(villages)),
            'distribution': {str(k): int(v) for k, v in villages.to_dict().items()}
        }
    
    # Check for duplicate rows
    duplicates = df.duplicated()
    if duplicates.sum() > 0:
        warnings.append(f"Duplicate rows found: {duplicates.sum()}")
    
    # Check data completeness by column type
    critical_cols = ['1.Ø§Ø³Ù… Ø§Ù„Ù…ÙØ³ØªØ¬ÙŠØ¨:', '4.Ø§Ù„Ù‚Ø±ÙŠØ©:', 'X', 'Y']
    for col in critical_cols:
        if col in df.columns:
            null_count = df[col].isna().sum()
            if null_count > 0:
                issues.append(f"Missing values in {col}: {null_count} rows")
    
    return issues, warnings, stats


def export_theme_csvs(df, theme_columns):
    """Export separate CSV files for each theme"""
    
    output_dir = Path('data/survey_by_theme')
    output_dir.mkdir(exist_ok=True)
    
    exported_files = []
    
    for theme, columns in theme_columns.items():
        if not columns:
            print(f"  âš ï¸  {theme}: No columns mapped, skipping")
            continue
        
        # Select only mapped columns that exist in dataframe
        available_cols = [col for col in columns if col in df.columns]
        
        if not available_cols:
            print(f"  âš ï¸  {theme}: No available columns in dataframe, skipping")
            continue
        
        theme_df = df[available_cols].copy()
        
        # Output files
        output_file = output_dir / f'MZSurvey_{theme}.csv'
        theme_df.to_csv(output_file, index=False, encoding='utf-8')
        
        exported_files.append({
            'theme': theme,
            'file': str(output_file),
            'rows': len(theme_df),
            'columns': len(theme_df.columns),
            'column_names': list(theme_df.columns)
        })
        
        print(f"  âœ“ {theme}: {len(theme_df)} rows Ã— {len(theme_df.columns)} columns â†’ {output_file.name}")
    
    return exported_files


def generate_mapping_report(theme_columns, unmapped_columns, issues, warnings, stats, exported_files):
    """Generate comprehensive mapping report"""
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'phase': 'Phase 1.4 - Column Mapping & Data Quality',
        'summary': {
            'total_themes': len(theme_columns),
            'mapped_columns': sum(len(cols) for cols in theme_columns.values()),
            'unmapped_columns': len(unmapped_columns),
            'data_quality_issues': len(issues),
            'data_quality_warnings': len(warnings),
            'exported_files': len(exported_files)
        },
        'theme_mapping': {
            theme: {
                'column_count': len(cols),
                'columns': cols
            } for theme, cols in theme_columns.items()
        },
        'unmapped_columns': unmapped_columns,
        'data_quality': {
            'issues': issues,
            'warnings': warnings,
            'statistics': stats
        },
        'exported_files': exported_files
    }
    
    output_file = Path('data/column_mapping_report.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ Mapping report saved: {output_file}")
    
    return report


def main():
    print("="*80)
    print("PHASE 1.4: COLUMN MAPPING & DATA QUALITY VALIDATION")
    print("="*80)
    
    # 1. Load data
    print("\nğŸ“Š Step 1: Loading survey data...")
    df = load_survey_with_coords()
    
    # 2. Map columns to themes
    print("\nğŸ—ºï¸  Step 2: Mapping columns to themes...")
    theme_columns, unmapped_columns = map_columns_to_themes(df)
    
    for theme, columns in theme_columns.items():
        print(f"  {theme}: {len(columns)} columns")
    
    if unmapped_columns:
        print(f"  âš ï¸  Unmapped: {len(unmapped_columns)} columns")
    
    # 3. Validate data quality
    print("\nâœ… Step 3: Validating data quality...")
    issues, warnings, stats = validate_data_quality(df)
    
    if issues:
        print("  âŒ Issues found:")
        for issue in issues:
            print(f"    - {issue}")
    else:
        print("  âœ“ No critical issues")
    
    if warnings:
        print("  âš ï¸  Warnings:")
        for warning in warnings:
            print(f"    - {warning}")
    
    # 4. Export theme-specific CSVs
    print("\nğŸ’¾ Step 4: Exporting theme-specific CSV files...")
    exported_files = export_theme_csvs(df, theme_columns)
    
    # 5. Generate report
    print("\nğŸ“ Step 5: Generating mapping report...")
    report = generate_mapping_report(theme_columns, unmapped_columns, issues, warnings, stats, exported_files)
    
    # Summary
    print("\n" + "="*80)
    print("âœ… PHASE 1.4 COMPLETE")
    print("="*80)
    print(f"  ğŸ“Š Themes mapped: {len(theme_columns)}")
    print(f"  ğŸ“ Files exported: {len(exported_files)}")
    print(f"  âœ“ Data quality: {len(issues)} issues, {len(warnings)} warnings")
    
    if stats.get('coordinates'):
        coord_stats = stats['coordinates']
        print(f"  ğŸ“ Coordinates: {coord_stats['with_coords']}/{coord_stats['total_rows']} rows")
    
    if stats.get('villages'):
        village_stats = stats['villages']
        print(f"  ğŸ˜ï¸  Villages: {village_stats['unique_count']} unique")
    
    print("="*80)
    
    if issues:
        print("\nâŒ CRITICAL ISSUES DETECTED - REVIEW BEFORE PROCEEDING")
        return False
    else:
        print("\nğŸš€ READY FOR PHASE 2: Integration with Existing Data")
        return True


if __name__ == '__main__':
    success = main()
    if not success:
        exit(1)
